{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-24T14:27:27.365497Z","iopub.execute_input":"2024-08-24T14:27:27.366040Z","iopub.status.idle":"2024-08-24T14:27:27.855122Z","shell.execute_reply.started":"2024-08-24T14:27:27.365980Z","shell.execute_reply":"2024-08-24T14:27:27.853774Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/creditcardfraud/creditcard.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"file_path = '/kaggle/input/creditcardfraud/creditcard.csv'","metadata":{"execution":{"iopub.status.busy":"2024-08-24T14:27:27.857233Z","iopub.execute_input":"2024-08-24T14:27:27.857751Z","iopub.status.idle":"2024-08-24T14:27:27.863099Z","shell.execute_reply.started":"2024-08-24T14:27:27.857691Z","shell.execute_reply":"2024-08-24T14:27:27.861928Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T14:27:27.864675Z","iopub.execute_input":"2024-08-24T14:27:27.865057Z","iopub.status.idle":"2024-08-24T14:27:32.669083Z","shell.execute_reply.started":"2024-08-24T14:27:27.865016Z","shell.execute_reply":"2024-08-24T14:27:32.667576Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T14:27:32.671740Z","iopub.execute_input":"2024-08-24T14:27:32.672210Z","iopub.status.idle":"2024-08-24T14:27:32.721734Z","shell.execute_reply.started":"2024-08-24T14:27:32.672163Z","shell.execute_reply":"2024-08-24T14:27:32.720273Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T14:27:32.723537Z","iopub.execute_input":"2024-08-24T14:27:32.724060Z","iopub.status.idle":"2024-08-24T14:27:32.775404Z","shell.execute_reply.started":"2024-08-24T14:27:32.724002Z","shell.execute_reply":"2024-08-24T14:27:32.774000Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming your dataset is in a pandas DataFrame called 'df'\n# and the target column is named 'target'\n\n# Separate features and target\nX = df.drop(columns=['Class'])\ny = df['Class']\n\n# Stratified split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# The 'stratify=y' argument ensures the proportion of 0s and 1s is the same in both train and test sets.\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T14:27:37.131459Z","iopub.execute_input":"2024-08-24T14:27:37.132074Z","iopub.status.idle":"2024-08-24T14:27:37.367108Z","shell.execute_reply.started":"2024-08-24T14:27:37.132029Z","shell.execute_reply":"2024-08-24T14:27:37.365807Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X.columns.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-24T14:27:38.268938Z","iopub.execute_input":"2024-08-24T14:27:38.269757Z","iopub.status.idle":"2024-08-24T14:27:38.276924Z","shell.execute_reply.started":"2024-08-24T14:27:38.269705Z","shell.execute_reply":"2024-08-24T14:27:38.275565Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(30,)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Script","metadata":{}},{"cell_type":"code","source":"import numpy as np \n\nclass Layer_Dense:\n    def __init__(self, n_inputs, n_neurons):\n        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n    def forward(self, inputs):\n        self.output = np.dot(inputs, self.weights) + self.biases\n\n\nclass Activation_ReLU:\n    def forward(self, inputs):\n        self.output = np.maximum(0, inputs)\n\nclass Activation_Softmax:\n    def forward(self, inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\nclass Loss:\n    def calculate(self, output, y):\n        sample_losses = self.forward(output, y)\n        data_loss = np.mean(sample_losses)\n        return data_loss\n\nclass Loss_CategoricalCrossentropy(Loss):\n    def forward(self, y_pred, y_true):\n        samples = len(y_pred)\n        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n\n        if len(y_true.shape) == 1:\n            correct_confidences = y_pred_clipped[range(samples), y_true]\n\n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n\n        negative_log_likelihoods = -np.log(correct_confidences)\n        return negative_log_likelihoods\n\n\n\n\n\ny_train_one_hot = np.eye(2)[y_train]\n\ndense1 = Layer_Dense(30, 64) \nactivation1 = Activation_ReLU()\n\ndense2 = Layer_Dense(64, 2)  \nactivation2 = Activation_Softmax()\n\n\ndense1.forward(X_train)\nactivation1.forward(dense1.output)\n\ndense2.forward(activation1.output)\nactivation2.forward(dense2.output)\n\n\nloss_function = Loss_CategoricalCrossentropy()\nloss = loss_function.calculate(activation2.output, y_train)\n\nprint(\"Loss:\", loss)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T14:27:41.544347Z","iopub.execute_input":"2024-08-24T14:27:41.544967Z","iopub.status.idle":"2024-08-24T14:27:42.049067Z","shell.execute_reply.started":"2024-08-24T14:27:41.544904Z","shell.execute_reply":"2024-08-24T14:27:42.046758Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Loss: 0.03306430331479273\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# with epochs and learning rate","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nclass Layer_Dense:\n    def __init__(self, n_inputs, n_neurons):\n        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n    def forward(self, inputs):\n        self.inputs = inputs  \n        self.output = np.dot(inputs, self.weights) + self.biases\n    def backward(self, dvalues):\n        self.dweights = np.dot(self.inputs.T, dvalues)\n        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n        self.dinputs = np.dot(dvalues, self.weights.T)\n    def update(self, learning_rate):\n        self.weights -= learning_rate * self.dweights\n        self.biases -= learning_rate * self.dbiases\n\nclass Activation_ReLU:\n    def forward(self, inputs):\n        self.inputs = inputs  \n        self.output = np.maximum(0, inputs)\n    def backward(self, dvalues):\n        self.dinputs = dvalues.copy()\n        self.dinputs[self.inputs <= 0] = 0\n\nclass Activation_Softmax:\n    def forward(self, inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n    def backward(self, dvalues):\n        self.dinputs = dvalues\n\nclass Loss_CategoricalCrossentropy:\n    def forward(self, y_pred, y_true):\n        samples = len(y_pred)\n        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n        if len(y_true.shape) == 1:\n            correct_confidences = y_pred_clipped[range(samples), y_true]\n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n        negative_log_likelihoods = -np.log(correct_confidences)\n        return np.mean(negative_log_likelihoods)\n    def backward(self, dvalues, y_true):\n        samples = len(dvalues)\n        if len(y_true.shape) == 1:\n            y_true = np.eye(dvalues.shape[1])[y_true]\n        self.dinputs = -y_true / dvalues\n        self.dinputs = self.dinputs / samples\n\n\ndef train(X_train, y_train, epochs, learning_rate):\n    dense1 = Layer_Dense(30, 64)\n    activation1 = Activation_ReLU()\n    dense2 = Layer_Dense(64, 2)\n    activation2 = Activation_Softmax()\n    loss_function = Loss_CategoricalCrossentropy()\n\n    for epoch in range(epochs):\n        \n        dense1.forward(X_train)\n        activation1.forward(dense1.output)\n        dense2.forward(activation1.output)\n        activation2.forward(dense2.output)\n\n       \n        loss = loss_function.forward(activation2.output, y_train)\n\n        \n        loss_function.backward(activation2.output, y_train)\n        dense2.backward(loss_function.dinputs)\n        activation1.backward(dense2.dinputs)\n        dense1.backward(activation1.dinputs)\n\n       \n        dense1.update(learning_rate)\n        dense2.update(learning_rate)\n\n        \n        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}')\n\n    return dense1, dense2, activation1, activation2, loss_function\n\n\ny_train_one_hot = np.eye(2)[y_train]\n\n\ndense1, dense2, activation1, activation2, loss_function = train(X_train, y_train_one_hot, epochs=30, learning_rate=0.01)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T14:53:40.532900Z","iopub.execute_input":"2024-08-24T14:53:40.533554Z","iopub.status.idle":"2024-08-24T14:54:02.853994Z","shell.execute_reply.started":"2024-08-24T14:53:40.533476Z","shell.execute_reply":"2024-08-24T14:54:02.852813Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1/30, Loss: 15.358772\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3916357868.py:53: RuntimeWarning: divide by zero encountered in divide\n  self.dinputs = -y_true / dvalues\n/tmp/ipykernel_37/3916357868.py:53: RuntimeWarning: invalid value encountered in divide\n  self.dinputs = -y_true / dvalues\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30, Loss: 0.027872\nEpoch 3/30, Loss: nan\nEpoch 4/30, Loss: nan\nEpoch 5/30, Loss: nan\nEpoch 6/30, Loss: nan\nEpoch 7/30, Loss: nan\nEpoch 8/30, Loss: nan\nEpoch 9/30, Loss: nan\nEpoch 10/30, Loss: nan\nEpoch 11/30, Loss: nan\nEpoch 12/30, Loss: nan\nEpoch 13/30, Loss: nan\nEpoch 14/30, Loss: nan\nEpoch 15/30, Loss: nan\nEpoch 16/30, Loss: nan\nEpoch 17/30, Loss: nan\nEpoch 18/30, Loss: nan\nEpoch 19/30, Loss: nan\nEpoch 20/30, Loss: nan\nEpoch 21/30, Loss: nan\nEpoch 22/30, Loss: nan\nEpoch 23/30, Loss: nan\nEpoch 24/30, Loss: nan\nEpoch 25/30, Loss: nan\nEpoch 26/30, Loss: nan\nEpoch 27/30, Loss: nan\nEpoch 28/30, Loss: nan\nEpoch 29/30, Loss: nan\nEpoch 30/30, Loss: nan\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing epochs","metadata":{}},{"cell_type":"code","source":"train(X_train, y_train_one_hot, epochs=10, learning_rate=0.001)\ntrain(X_train, y_train_one_hot, epochs=30, learning_rate=0.01)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T14:54:02.856352Z","iopub.execute_input":"2024-08-24T14:54:02.857370Z","iopub.status.idle":"2024-08-24T14:54:34.033329Z","shell.execute_reply.started":"2024-08-24T14:54:02.857288Z","shell.execute_reply":"2024-08-24T14:54:34.031812Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 13.981863\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3916357868.py:53: RuntimeWarning: divide by zero encountered in divide\n  self.dinputs = -y_true / dvalues\n/tmp/ipykernel_37/3916357868.py:53: RuntimeWarning: invalid value encountered in divide\n  self.dinputs = -y_true / dvalues\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.027872\nEpoch 3/10, Loss: nan\nEpoch 4/10, Loss: nan\nEpoch 5/10, Loss: nan\nEpoch 6/10, Loss: nan\nEpoch 7/10, Loss: nan\nEpoch 8/10, Loss: nan\nEpoch 9/10, Loss: nan\nEpoch 10/10, Loss: nan\nEpoch 1/30, Loss: 15.417512\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3916357868.py:53: RuntimeWarning: divide by zero encountered in divide\n  self.dinputs = -y_true / dvalues\n/tmp/ipykernel_37/3916357868.py:53: RuntimeWarning: invalid value encountered in divide\n  self.dinputs = -y_true / dvalues\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30, Loss: 0.027872\nEpoch 3/30, Loss: nan\nEpoch 4/30, Loss: nan\nEpoch 5/30, Loss: nan\nEpoch 6/30, Loss: nan\nEpoch 7/30, Loss: nan\nEpoch 8/30, Loss: nan\nEpoch 9/30, Loss: nan\nEpoch 10/30, Loss: nan\nEpoch 11/30, Loss: nan\nEpoch 12/30, Loss: nan\nEpoch 13/30, Loss: nan\nEpoch 14/30, Loss: nan\nEpoch 15/30, Loss: nan\nEpoch 16/30, Loss: nan\nEpoch 17/30, Loss: nan\nEpoch 18/30, Loss: nan\nEpoch 19/30, Loss: nan\nEpoch 20/30, Loss: nan\nEpoch 21/30, Loss: nan\nEpoch 22/30, Loss: nan\nEpoch 23/30, Loss: nan\nEpoch 24/30, Loss: nan\nEpoch 25/30, Loss: nan\nEpoch 26/30, Loss: nan\nEpoch 27/30, Loss: nan\nEpoch 28/30, Loss: nan\nEpoch 29/30, Loss: nan\nEpoch 30/30, Loss: nan\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(<__main__.Layer_Dense at 0x7e547fb6b520>,\n <__main__.Layer_Dense at 0x7e547fbb2320>,\n <__main__.Activation_ReLU at 0x7e547fbb35e0>,\n <__main__.Activation_Softmax at 0x7e549450dfc0>,\n <__main__.Loss_CategoricalCrossentropy at 0x7e547fb98eb0>)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}